{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic data\n",
    "\n",
    "<img src=\"https://media4.giphy.com/media/XOY5y7YXjTD7q/giphy.gif?cid=ecf05e473tvjfnpeburx7eq75c81fxuxc7dtrn89jo61ftih&ep=v1_gifs_search&rid=giphy.gif&ct=g\" style=\"width=400;height=300\">\n",
    "\n",
    "\n",
    "We are going to use the Kaggle's Titanic dataset to illustrate binary classificaiton using a **random forest model**. You can get more information on the dataset [here](https://www.kaggle.com/competitions/titanic/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import IFrame # Used to display a webpage\n",
    "from IPython.display import Image # Used to display an image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Read the data\n",
    "#df = pd.read_csv('titanic.csv')\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a description of what the variables included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"data/titanic_codebook.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "Rarely, is a dataset \"perfect\" when you load it in for analysis -- some preprocessing is almost always necessary. First, it's important to note that `sklearn`'s random forest classifier cannot handle missing data, so either need to remove `NA`'s prior to analysis (i.e., list-wise deletion) or impute missing values. If we look at the `.describe()` method for our `pandas` dataframe, we see that `Age` has missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = df['Age'].isnull().sum()\n",
    "print(f\"Number of missing values in Age column: {missing_values_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use \"mean imputation\" to fill in these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in missing values for age\n",
    "mean_age = df['Age'].mean()\n",
    "\n",
    "# Fill missing values in the Age column with the mean\n",
    "df['Age'] = df['Age'].fillna(mean_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Sex` of the passanger is also likely to be an important variable and it is currently saved as a `string` (or an `object` in `pandas` speak):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sex'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a **dummy variable** for whether a passanger is a female by using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['female'] = (df['Sex'] == 'female').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all the preprocessing that we will do for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into training and testing sets\n",
    "\n",
    "As per usual, we are interested in out-of-sample performance not in-sample fit. Let's split our data into **training** and **testing** sets before going any further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into traning and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "Feature selection is just a francy name for deciding what variables to include in the model. Note that we could just throw in every variable included in the dataset (aka, the \"kitchen sink\"), but we'll select a smaller subset of the available variables just to keep things simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features\n",
    "features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'female']\n",
    "y = 'Survived'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The components of a random forest: decision trees and bootstrapping\n",
    "\n",
    "As described in lecture, **random forests** are constructed by combining multiple **decision trees** using **bootstrap aggegation**. Note that `sklearn` will carryout all of these steps for you when using the `RandomForestClassifer()` class; however, let's take a closer look at the two most important components: **decision trees** and **bootstrapping**\n",
    "\n",
    "\n",
    "### Decision tree classificaiton in `sklearn`\n",
    "\n",
    "Fitting an indvidual decision tree in `sklearn` follow the same syntax as fitting any other model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# create a decision tree classifier\n",
    "clf = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "# train the classifier\n",
    "clf.fit(df_train[features], df_train[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can visualize the actual decision tree constructed using the `DecisionTreeClassifer` class by typing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(clf, feature_names=features, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good thing that we don't need to actually calculate this by hand! Once we have our model, we can use it to `predict` new data and assess out-of-sample performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred = clf.predict(df_test[features])\n",
    "\n",
    "# evaluate the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(df_test[y], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, one of the best features (no pun intended) of decision trees (and by extension random forests) is they are easy to intrepret. For example, we can look at which of our features are teh most important to the overall prediction by calling the `.feature_importances_` [method](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot our feature importance for easy visualiszation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(features, clf.feature_importances_)\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use a slightly better visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features = sorted(zip(features, clf.feature_importances_), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Unzip the sorted features and their importances\n",
    "sorted_features, sorted_importances = zip(*sorted_features)\n",
    "\n",
    "# Plot the sorted feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(sorted_features, sorted_importances)\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.gca().invert_yaxis()  # Ensure the most important feature is at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping \"by hand\" with `pandas`\n",
    "\n",
    "Again, the `RandomForestClassifer()` class will do all the \"bootstrap aggregating\" for you. However, the concept of generating bootstrap random samples is important beyond random forest models and it's good to take a quick look at bootrrapping in `Python`. There are many difference ways to bootstrap in `Python`, but we'll do it \"by hand\" using `pandas` to illustrate the main concepts.\n",
    "\n",
    "Say, for instance, we want to understand variation in the survival rate of passengers and we don't want to use standard formulas. How could we use bootstrapping instead?\n",
    "\n",
    "Let's start by defining how many sample we want and a list to hold our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of bootstrap samples to generate\n",
    "num_bootstrap_samples = 10000\n",
    "\n",
    "# Initialize a list to store the survival proportion of each bootstrap sample\n",
    "survival_proportions = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need repeat the following procedure `num_bootstrap_samples` times:\n",
    "1. Sample from our original data (`df`) with **replacement**\n",
    "2. Calculate the surivial proportion.\n",
    "3. Store the result in `survival_proportions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform bootstrapping\n",
    "for i in range(num_bootstrap_samples):\n",
    "    # Randomly sample with replacement from the original DataFrame\n",
    "    sample = df.sample(n=len(df), replace=True)\n",
    "    \n",
    "    # Add the sample estimate to the list of sample estimates\n",
    "    survival_proportions.append(sample['Survived'].mean())\n",
    "\n",
    "print('Here are the first 10 survival proportions:')\n",
    "print(survival_proportions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram to visualize the bootstrap estimates of the mean\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(survival_proportions, kde=True)\n",
    "plt.xlabel('Bootstrap Sample Mean')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bootstrap Estimates of the Mean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I was a super crabby professor, I would make you combine the above bootstrapping procedure and the `DecisionTreeClassifer()` class to create random forest classifer from scratch. However, I'm not a crabby professor. Onwards to the `RandomForestClassifer()`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forests classification in `sklearn`\n",
    "\n",
    "At the risk of sounding like a broken record, we fit a `RandomForestClassifer()` in `sklearn` using the same syntax as any other model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary class\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# create a random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, criterion='entropy')\n",
    "\n",
    "# train the classifier\n",
    "clf.fit(df_train[features], df_train[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine the most important features for our random forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_features = sorted(zip(features, clf.feature_importances_), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Unzip the sorted features and their importances\n",
    "sorted_features, sorted_importances = zip(*sorted_features)\n",
    "\n",
    "# Plot the sorted feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(sorted_features, sorted_importances)\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.gca().invert_yaxis()  # Ensure the most important feature is at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can ``predict`` new data and assess out-of-sample performance in the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred = clf.predict(df_test[features])\n",
    "\n",
    "# evaluate the model\n",
    "accuracy_score(df_test[y], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification performance\n",
    "\n",
    "With our prediction in hand, we can now calculate a range of **performance metrics**. All of these metrics start with what's referred to as a **confusion matrix**: \n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/337071081/figure/fig2/AS:941941982236673@1601587877948/A-confusion-matrix-for-binary-classification.png\">\n",
    "\n",
    "From this matrix, we can define the following:\n",
    "\n",
    "\\begin{equation}\n",
    "    Acurracy = \\frac{TP + TN}{TP + FP + FN + TN}\n",
    "  \\end{equation}\n",
    "  \n",
    "  \\begin{equation}\n",
    "    Precision = \\frac{TP}{TP + FP}\n",
    "  \\end{equation}\n",
    "  \n",
    "  \\begin{equation}\n",
    "    Recall = \\frac{TP}{TP + FN}\n",
    "  \\end{equation}\n",
    "  \n",
    "  \\begin{equation}\n",
    "    Specificity = \\frac{TN}{TN + FP}\n",
    "  \\end{equation}\n",
    "\n",
    "  Or if you are a visual learner:\n",
    "\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\" style=\"width=300;height=400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we need alternative performance metrics?\n",
    "\n",
    "The most commonly employed metric is **accuracy**. However, we need to be very careful when using accuracy alone. For example, consider Dallas Raines, the most accurate meteorologist in history...\n",
    "\n",
    "<img src=\"http://farm6.static.flickr.com/5260/5516412091_06fea7fdb8.jpg\" style=\"width=400;height=300\">\n",
    "\n",
    "If accuracy falls apart for \"imbalanced classes,\" then what are our alternatives. The most common alternatives (and what I tend to use in my own work) is some combination of **precision** and **recall**, which is referred to as the **F1-score**.\n",
    "\n",
    "Specifically, these measures are formally combined in the **F1-score**:\n",
    "\n",
    "\\begin{equation}\n",
    "  F1 = \\frac{2\\:x\\:Precision\\:x\\:Recall}{Precision\\:+\\:Recall}\n",
    "\\end{equation}\n",
    "\n",
    "The F1-score takes into account the tradeoff between precision and accuracy. \n",
    "\n",
    "All of these metrics (and others as well) are easy to calculate in `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary functions\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# calculate precision\n",
    "print('precision = ', precision_score(df_test[y], y_pred))\n",
    "\n",
    "# calculate recall\n",
    "print('recall = ', recall_score(df_test[y], y_pred))\n",
    "\n",
    "# calculate f1-score\n",
    "print('F1 = ', f1_score(df_test[y], y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing out-of-sample performance with cross-validation\n",
    "\n",
    "So far, we've randomly split our data into a single training and testing set using the `train_test_split` function. The realized sample, however, is just one of many samples that we could have pulled. Here, we will look at **k-fold cross-validation**. What do we mean be cross-validation? Take a look at the following:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg\">\n",
    "\n",
    "Once again, cross-validation is easy in `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# estimate cross-validation accuracy\n",
    "cv_scores = cross_val_score(clf, df_train[features], df_train[y], cv=5)\n",
    "print('CV accuracy scores:', cv_scores)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (cv_scores.mean(), cv_scores.std()))\n",
    "\n",
    "# estimate cross-validation precision\n",
    "cv_scores = cross_val_score(clf, df_train[features], df_train[y], cv=5, scoring='precision')\n",
    "print('CV precision scores:', cv_scores)\n",
    "print('CV precision: %.3f +/- %.3f' % (cv_scores.mean(), cv_scores.std()))\n",
    "\n",
    "# estimate cross-validation recall\n",
    "cv_scores = cross_val_score(clf, df_train[features], df_train[y], cv=5, scoring='recall')\n",
    "print('CV recall scores:', cv_scores)\n",
    "print('CV recall: %.3f +/- %.3f' % (cv_scores.mean(), cv_scores.std()))\n",
    "\n",
    "# estimate cross-validation f1-score\n",
    "cv_scores = cross_val_score(clf, df_train[features], df_train[y], cv=5, scoring='f1')\n",
    "print('CV f1 scores:', cv_scores)\n",
    "print('CV f1: %.3f +/- %.3f' % (cv_scores.mean(), cv_scores.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "We haven't really addressed so-called \"hyperparameters\" yet, but \"tuning\" these parameters can often really help your model's performance. What's a **hyperparameter**? According to [Wikipedia](https://en.wikipedia.org/wiki/Hyperparameter_machine_learning),\n",
    "\n",
    "> In machine learning, a hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are derived via training.\n",
    "\n",
    "Here's a list of all of the **hyperparameters** for our `RandomForestClassifer()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame('https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html', width=700, height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obvious next question: which of these **hyperparameters** actually matter? In turns out that the `n_estimators` and the `max_features` hyperparameters can really impact learning. Let's start with `n_estimators`.\n",
    "\n",
    "There are two ways for \"optimizing hyperparameters\" in `sklearn`: `GridSearchCV` and `RandomizedSearchCV`. Let's start by looking at `GridSearch()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Save a list of dicts with the hyperparameters to test\n",
    "param_grid = [\n",
    "  {'n_estimators': list(range(100, 1000, 100))}\n",
    " ]\n",
    "\n",
    "# create a random forest classifier\n",
    "clf = RandomForestClassifier(criterion='entropy', random_state=42)\n",
    "\n",
    "# create grid search object\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='f1', return_train_score=True, n_jobs=4)\n",
    "\n",
    "# run the grid search\n",
    "grid_search.fit(df_train[features], df_train[y])\n",
    "\n",
    "print('Best parameters:\\n', grid_search.best_params_)\n",
    "print('\\nBest score = ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can actually use the fitted `grid_search` object to make predictions using the \"best\" model determined from our grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_pred = grid_search.predict(df_test[features])\n",
    "\n",
    "# evaluate the model\n",
    "f1_score(df_test[y], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add additional hyperparameters to assess by adding more entries to our `param_grid` list of dicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'n_estimators': list(range(100, 1000, 100)),\n",
    "   'max_features': ['log2', 'sqrt']}\n",
    " ]\n",
    "\n",
    "clf = RandomForestClassifier(criterion='entropy', random_state=42)\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='f1', return_train_score=True, n_jobs=4)\n",
    "grid_search.fit(df_train[features], df_train[y])\n",
    "\n",
    "print('Best parameters:\\n', grid_search.best_params_)\n",
    "print('\\nBest score = ', grid_search.best_score_)\n",
    "\n",
    "# make predictions\n",
    "y_pred = grid_search.predict(df_test[features])\n",
    "\n",
    "# evaluate the model\n",
    "print('\\nTesting F1-score = ', f1_score(df_test[y], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with `GridSearchCV` is that it only works for searching small parameter spaces. If we need to search lot's of different hyperparameters, then we need to turn to `RandomSearchCV`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = [\n",
    "  {'n_estimators': list(range(100, 1000, 100)),\n",
    "   'max_features': ['log2', 'sqrt'],\n",
    "   'criterion': ['gini', 'entropy', 'log_loss']\n",
    "   }\n",
    " ]\n",
    "\n",
    "clf = RandomForestClassifier(criterion='entropy', random_state=42)\n",
    "grid_search = RandomizedSearchCV(clf, param_grid, cv=5, scoring='f1', return_train_score=True, n_jobs=4)\n",
    "grid_search.fit(df_train[features], df_train[y])\n",
    "\n",
    "print('Best parameters:\\n', grid_search.best_params_)\n",
    "print('\\nBest score = ', grid_search.best_score_)\n",
    "\n",
    "# make predictions\n",
    "y_pred = grid_search.predict(df_test[features])\n",
    "\n",
    "# evaluate the model\n",
    "print('\\nTesting F1-score = ', f1_score(df_test[y], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for now. We will discuss an additional approach to hyperparameter tuning -- i.e., \"Bayesian optimization\" -- in later weeks, but this is everything that you need to get started.\n",
    "\n",
    "<img src=\"https://media0.giphy.com/media/iPNq9rFkAIVgs/giphy.webp?cid=ecf05e477kqr72weerxpbg9w032pzy30vzrlui4wnxt4xlrh&ep=v1_gifs_search&rid=giphy.webp&ct=g\" style=\"width=400;height=300\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
