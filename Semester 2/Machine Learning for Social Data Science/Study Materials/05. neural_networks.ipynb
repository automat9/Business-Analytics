{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08919cbb",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks in Python\n",
    "\n",
    "The lecture slides have provided an overview of neural networks, including key concepts such as network **layers**, the different types of **nodes** (or neurons), and **weights**, while also touching on importance concepts like **activiation functions**. We are now going to go from concepts to practice by learning how to build, train, and test your own neural network in Python using the ``tensorflow`` and ``keras`` libraries.\n",
    "\n",
    "Let's start by importing several key libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c441394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841db761",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We are going to continue with our Titanic data example. To refresh, our goal is to predict passenger survival (1 for survived, 0 for not survived) based on the following variables:\n",
    "* `Pclass`: Passenger class (1 = 1st, 2 = 2nd, 3 = 3rd).\n",
    "* `Age`: Age of the passenger.\n",
    "* `SibSp`: Number of siblings/spouses aboard.\n",
    "* `Parch`: Number of parents/children aboard.\n",
    "* `Fare`: Ticket fare paid.\n",
    "* `female`: Binary variable indicating if the passenger is female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da34d632-5bb2-4903-ae19-9c1d32e57b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df = pd.read_csv('data/titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd860e-5979-4ea2-8e58-a4d8c2c5a597",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "We'll do just a little bit of pre-processing to the data prior to training our NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48291fe1-6f5f-4612-8856-d6e618b805ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "\n",
    "# Normalize features. This just helps the model converge faster\n",
    "scaler = StandardScaler()\n",
    "df[['Age', 'Fare']] = scaler.fit_transform(df[['Age', 'Fare']])\n",
    "\n",
    "# Encode female variable\n",
    "df['female'] = df['Sex'].apply(lambda x: 1 if x == 'female' else 0)\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'female']]\n",
    "y = df['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97311d59-181b-46a2-b7d8-0a0c651d7406",
   "metadata": {},
   "source": [
    "## Neural network architecture\n",
    "\n",
    "With our data ready to go, it's now time to build our simple neural network. We'll go with the following model for the time being:\n",
    "  - **Input Layer**: 6 nodes (1 for each feature: `Pclass`, `Age`, `SibSp`, `Parch`, `Fare`, `female`).\n",
    "  - **Hidden Layer**: 1 fully connected layer (i.e., \"dense\" layer) with 4 nodes with **ReLU (Rectified Linear Unit)** activation functions.\n",
    "  - **Output Layer**: 1 node with a **sigmoid activation function** for binary classification (survival or not).\n",
    "  - **Loss Function**: Binary cross-entropy for classification problems.\n",
    "  - **Optimizer**: Adam optimizer for training.\n",
    "\n",
    "It's quite easy to build this model in `Python` using `keras`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fda6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Input(shape=(6,)),              # Define the input shape explicitly  \n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c30a9",
   "metadata": {},
   "source": [
    "Here's on overview of the key **concepts** using in our architecture:\n",
    "\n",
    "- **Sequential**: This is a linear stack of layers in Keras. It allows you to build a model layer by layer, where each layer has exactly one input tensor and one output tensor.\n",
    "\n",
    "- **Dense**: This layer is a fully connected layer, meaning each neuron in the layer is connected to every neuron in the previous layer. This is what we used in lecture.\n",
    "\n",
    "- **ReLU (Rectified Linear Unit)**: This is an activation function that is defined as the positive part of its input. It is one of the most popular activation functions used in neural networks because it helps to mitigate the vanishing gradient problem. More on this below!\n",
    "\n",
    "- **Sigmoid**: This is an activation function that outputs a value between 0 and 1. It is often used in the output layer of binary classification problems because it can be interpreted as a probability. This is what we use for logistic regression, so should look familiar.\n",
    "\n",
    "- **Adam**: This is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iteratively based on training data. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems. **You can think of Adam as a smarter version of the gradient descent algorithm that we discussed in week 1**.\n",
    "\n",
    "- **Binary Cross-Entropy**: This is a loss function used for binary classification problems. It measures the performance of a classification model whose output is a probability value between 0 and 1. The binary cross-entropy loss increases as the predicted probability diverges from the actual label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362db2b1",
   "metadata": {},
   "source": [
    "### Activiation functions\n",
    "\n",
    "As we discussed in lecture, **activiation function**s decide which neurons \"fire\" when moving though the network and add nonlinearity into our network. Without activation functions, the entire neural network would behave like a single linear transformation, no matter how many layers it has. This setup would not provide the necessary complexity to help with real-world problems.\n",
    "\n",
    "Some common activation functions:\n",
    "   - **ReLU (Rectified Linear Unit)**: $$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "   - **Sigmoid**: $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "   - **Tanh**: $$\\text{tanh}(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "#### Why ReLU?\n",
    "\n",
    "ReLU is a very (very!) common activiation used for hidden layers. Here are some reasons why:\n",
    "   - **Simplicity**: ReLU is computationally efficient compared to other activation functions like Sigmoid or Tanh.\n",
    "   - **Prevents Vanishing Gradients**:\n",
    "     - Gradients in Sigmoid or Tanh functions can become very small for large input values, slowing down learning.\n",
    "     - ReLU helps maintain larger gradients, speeding up training.\n",
    "   - **Effective in Deep Networks**: It works well for deep networks by introducing sparsity (many neurons output 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c11cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ReLU function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Input values\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y_relu = relu(x)\n",
    "\n",
    "# Plot ReLU\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y_relu, label='ReLU', color='blue')\n",
    "plt.axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "plt.axvline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "plt.title('ReLU Activation Function')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c8c05f",
   "metadata": {},
   "source": [
    "### Training our model\n",
    "\n",
    "That's it for setting up our model. Now we can train our model using the model's `fit()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aabfbd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7fc3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4c5b1",
   "metadata": {},
   "source": [
    "When training a neural network, two important hyperparameters to understand are **epochs** and **batch size**.\n",
    "\n",
    "- **Epochs**: An epoch refers to one complete pass through the entire training dataset. During each epoch, the model processes every training example once. Training for more epochs generally improves the model's performance, but too many epochs can lead to overfitting, where the model performs well on the training data but poorly on unseen data.\n",
    "\n",
    "- **Batch Size**: The batch size is the number of training examples processed before the model's internal parameters are updated. Instead of updating the model's parameters after each training example, which can be computationally expensive, the model updates its parameters after processing a batch of examples. Smaller batch sizes can lead to more accurate updates but require more iterations to complete an epoch, while larger batch sizes can speed up training but may lead to less accurate updates.\n",
    "\n",
    "| `batch_size`  | **Effect** |\n",
    "|--------------|-----------|\n",
    "| **Small (e.g., 16, 32)**  | More updates, better generalization, but **slower training**. |\n",
    "| **Large (e.g., 128, 256, 512)**  | Fewer updates, **faster training**, but may generalize worse. |\n",
    "| **Full Batch (`batch_size=len(X_train)`)** | One update per epoch (**slow convergence, may get stuck**). |\n",
    "\n",
    "Choosing the right number of epochs and batch size is crucial for training an effective neural network. Too few epochs can result in underfitting, while too many can cause overfitting. Similarly, the batch size can affect the stability and speed of the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dab4a0",
   "metadata": {},
   "source": [
    "### Making predictions\n",
    "\n",
    "With our fitted model in hand, we can now calculate predicted probablities, get predictions, and examine out-of-sample performance using our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74db7a4-20e9-47f8-a265-6d78f116e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predict method outputs the probability of survival\n",
    "probs = model.predict(X_test)\n",
    "\n",
    "# Convert the probabilities to binary predictions\n",
    "predictions = (probs > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "print(f'Here are the first 10 predications: {predictions[0:10]}')\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934a669",
   "metadata": {},
   "source": [
    "To avoid having to repeat the code in the previous cell each time that we want to assess model performance, let's create a function that we can reuse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d547094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(model, X_test, y_test):\n",
    "    # The predict method outputs the probability of survival\n",
    "    probs = model.predict(X_test)\n",
    "\n",
    "    # Convert the probabilities to binary predictions\n",
    "    predictions = (probs > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89271559",
   "metadata": {},
   "source": [
    "## Making our network deep(er)\n",
    "\n",
    "The example we've used so far is very, very simple and doesn't really demonstrate the power of NN's for solving complex machine learning tasks. Let's add an additional hidden layer to allow our model to learn more complex patterns in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442e0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Input(shape=(6,)),\n",
    "    Dense(16, activation='relu'),  # Add a hidden layer!    \n",
    "    Dense(8, activation='relu'),  # Add a hidden layer!           \n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=25, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5800e48d",
   "metadata": {},
   "source": [
    "How does our more complex model perform? Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c2f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_performance(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22563c4",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout is a **regularization technique** used to prevent overfitting in neural networks. During training, dropout randomly sets a fraction of the input units to zero at each update. This prevents the network from becoming too reliant on any particular neurons and encourages the network to learn more robust features. Dropout can be applied to both input and hidden layers.\n",
    "\n",
    "Key points about dropout:\n",
    "- **Randomly drops neurons**: During each training iteration, a random subset of neurons is ignored (dropped out).\n",
    "- **Reduces overfitting**: By preventing neurons from co-adapting too much, dropout helps in reducing overfitting.\n",
    "- **Improves generalization**: Dropout forces the network to learn more general features that are useful across different subsets of data.\n",
    "\n",
    "Regularization techniques are big topic and a full account of these techniques is outside of the scope of this module. In a nutshell, these techniques are used to prevent overfitting by adding a penalty to the loss function. This penalty discourages the model from fitting the noise in the training data and encourages simpler models that generalize better to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd14e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Input(shape=(6,)),\n",
    "    Dense(16, activation='relu'),  # Add a hidden layer!  \n",
    "    Dropout(0.2),  # Add dropout!  \n",
    "    Dense(8, activation='relu'),  # Add a hidden layer!\n",
    "    Dropout(0.2),  # Add dropout!          \n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=25, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "plot_loss(history)\n",
    "\n",
    "evaluate_performance(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42345f8",
   "metadata": {},
   "source": [
    "## The \"art\" of training a neural network\n",
    "\n",
    "As we've discussed, training a neural network is more art than science. How many epochs should we use? What's a good learning rate? What is a good batch size? Answering these -- and many other! -- questions are important aspects of training an neural network and there is rarely (if ever) a one size fits all approach.\n",
    "\n",
    "This section outlines a handful of techniques that I've found useful over the years when fitting these networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ad2b7",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "\n",
    "The `keras` library has a number of different `callbacks` that we can use to monitor training performance and make decisions on our behalf. Setting up an **early stopping** rule is one such `callback` that helps avoid using too many epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fdf2fa-3ed0-4ea3-9f47-6d7bcc3c1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Input(shape=(6,)),\n",
    "    Dense(16, activation='relu'),  # Add a hidden layer!  \n",
    "    Dropout(0.2),  # Add dropout!  \n",
    "    Dense(8, activation='relu'),  # Add a hidden layer!\n",
    "    Dropout(0.2),  # Add dropout!          \n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',       # Monitor the validation loss\n",
    "    patience=3,               # Number of epochs with no improvement to wait before stopping\n",
    "    restore_best_weights=True # Restore model weights to the best epoch\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,               # Set a high maximum number of epochs\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stopping] # Use early stopping\n",
    ")\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdacdbf",
   "metadata": {},
   "source": [
    "### Adjusting the learning rate\n",
    "\n",
    "The default learning rate for `keras` is `0.001`, which works pretty well a lot of the time. However, sometimes it helps to adjust this learning rate (up or down) depending on the dataset at hand. Here's an example Another very helpful `callback` starts with a large(ish) learning rate and adjusts the learning rate downwards based on performance. Here's an example of how to adjust the learning rate used by the `Adam` optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Adam optimizer\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Input(shape=(6,)),\n",
    "    Dense(16, activation='relu'),  # Add a hidden layer!  \n",
    "    Dropout(0.2),  # Add dropout!   \n",
    "    Dense(8, activation='relu'),  # Add a hidden layer!\n",
    "    Dropout(0.2),  # Add dropout!          \n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Define the Adam optimizer with a custom learning rate\n",
    "custom_adam = Adam(learning_rate=0.01)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=25, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a064c959",
   "metadata": {},
   "source": [
    "Another (and probably better!) way to adjust the learning rate is by using the `ReduceLROnPlateu` callback in `keras`. By using this callback, you can start with a larger learning rate for early epochs and decrease the learning rate for later epochs. Here's how you would implement this callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df488f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Input(shape=(6,)),\n",
    "    Dense(16, activation='relu'),  # Add a hidden layer!  \n",
    "    Dropout(0.2),  # Add dropout!  \n",
    "    Dense(8, activation='relu'),  # Add a hidden layer!\n",
    "    Dropout(0.2),  # Add dropout!          \n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Define the Adam optimizer with a custom learning rate\n",
    "custom_adam = Adam(learning_rate=0.01)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',       # Monitor the validation loss\n",
    "    factor=0.5,               # Reduce learning rate by half\n",
    "    patience=2,               # Wait for 2 epochs of no improvement\n",
    "    min_lr=.001,              # Minimum learning rate\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=25,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[reduce_lr]\n",
    ")\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051bef35",
   "metadata": {},
   "source": [
    "One really cool feature about using callbacks in `keras` is that they can be combined. For instance, we can systematically update our learning rate, while also setting up an early stopping rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5623d37a-e184-4d33-953f-3ee6a93de479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential([\n",
    "    Input(shape=(6,)),\n",
    "    Dense(16, activation='relu'),  # Add a hidden layer!  \n",
    "    Dropout(0.2),  # Add dropout!  \n",
    "    Dense(8, activation='relu'),  # Add a hidden layer!\n",
    "    Dropout(0.2),  # Add dropout!          \n",
    "    Dense(4, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Define the Adam optimizer with a custom learning rate\n",
    "custom_adam = Adam(learning_rate=0.005)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=custom_adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reduce LR when val_loss plateaus\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,            # Reduce LR by half\n",
    "    patience=3,            # Wait 3 epochs of no improvement\n",
    "    min_lr=1e-6,           # Minimum allowed LR\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Stop training early if val_loss does not improve\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,            # Stop after 6 epochs of no improvement\n",
    "    restore_best_weights=True,  # Restore best weights before stopping\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model with both callbacks\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507af48a",
   "metadata": {},
   "source": [
    "## Resources on training neural networks in Python\n",
    "\n",
    "The purpose of this week was to introduce you to the key concepts of neural networks and to show you how these models can be trained in practice. In this class (and probably in your careers!), we are rarely going to build a neural network from scratch, but instead use **transfer learning** to efficiently fine neural networks created by others to work for our specific tasks.\n",
    "\n",
    "If, however, you wanted to know more about training NNs in Python, there are a TON of resources online. Here are some of my favourites:\n",
    "\n",
    "- Francois Chollet's book, [Deep Learning with Python](https://sourestdeeds.github.io/pdf/Deep%20Learning%20with%20Python.pdf)\n",
    "- If YouTube is more your thing, then take a look at [Neural networks playlist](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) created by 3Blue1Brown.\n",
    "- And many, many more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
