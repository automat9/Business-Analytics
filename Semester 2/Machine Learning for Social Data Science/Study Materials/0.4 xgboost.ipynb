{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme boosted trees (XGBoost) for classification (and regression)\n",
    "\n",
    "Let's take a look at how to train an `xgboost` model for classificaiton in Python using the Titanic dataset from last week. First, load the Titanic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # we'll need this later!\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('data/titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `xgboost` library is not installed by default in `Anaconda`, so you will need to use `pip` to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now import the xgboost library in the typical way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing and feature selection\n",
    "\n",
    "When compared to the `RandomForestClassifer()` model, we need to do considerably less preprocessing in advance of model training (e.g., no imputation for missing data). However, `xgboost` will still complain if you try to pass `pandas` \"objects\" (i.e., strings) directly to your model. As such, let's recode the `Sex` variable to be an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Sex\n",
    "df['female'] = (df['Sex'] == 'female').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in order to compare `xgboost` to our `RandomForestClassifer()` from last week, let's use the same features (**note**: again, you could include all availalble variables in the dataset if you want to!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features\n",
    "features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'female']\n",
    "y = 'Survived'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into training and testing sets\n",
    "\n",
    "Again, to facilitate comparison, let's split our data into **training** and **testing** sets in the exact same way that we did last week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into traning and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost in Python\n",
    "\n",
    "We are now ready to fit our model. Given that we are interested in binary classification (survived vs. not survived), we start by setting up an `XGBClassifier` using a `binary:logistic` objective function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with `sklearn` models, we can train this model (using default hyperparmeters) using the `fit()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(df_train[features], df_train[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can assess out-of-sample performance in the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the metrics from sklearn\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(df_test[features])\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(df_test[y], y_pred)\n",
    "print(f'Precision is {precision}')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(df_test[y], y_pred)\n",
    "print(f'Recall is {recall}')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(df_test[y], y_pred)\n",
    "print(f'F1 score is {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning and Bayesian optimization\n",
    "\n",
    "So far, we've used the `XGBClassifier` default hyperparamaters. As with our random forest classifer, it's easy to change these hyperparmeters using the `xgboost` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's lower the learning rate\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', learning_rate = .05)\n",
    "\n",
    "# Fit the model\n",
    "xgb_model.fit(df_train[features], df_train[y])\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(df_test[features])\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(df_test[y], y_pred)\n",
    "print(f'Precision is {precision}')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(df_test[y], y_pred)\n",
    "print(f'Recall is {recall}')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(df_test[y], y_pred)\n",
    "print(f'F1 score is {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that already really helped in terms of performance! We could try to find an even better solution using either `sklearn`'s `GridSearchCV` or `RandomizedSearchCV` as demonstrated last week. However, these approaches are either impossible if you have a large \"hyperparmeter space\" (`GridSearchCV`) or extremely inefficient (`RandomizedSearchCV`). And the benefit of `xgboost` is it's flexibility: there are many hyperparmeters to choose from in order to find a model suitble for your data. What's a budding data scientist to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian optimization\n",
    "\n",
    "The answer: **Bayesian optimization**. The details of Bayesian optimization are quite complex and probably not necessary for us to undertand at this stage. But let's get a basic understanding of how Bayesian optimization  works.\n",
    "\n",
    "#### What is Bayesian Optimization?\n",
    "- Used for optimizing expensive-to-evaluate functions (e.g., machine learning hyperparameters).\n",
    "- Balances **exploration** (searching unknown areas) and **exploitation** (focusing on promising areas).\n",
    "\n",
    "**Key concepts** for understanding Bayesian optimization:\n",
    "- **Objective Function**: The function we want to minimize/maximize (e.g., a model's validation error).\n",
    "- **Probabilistic Model**: A surrogate model (like Gaussian Processes or Tree-structured Parzen Estimators) approximates the objective function.\n",
    "- **Acquisition Function**: Guides the next point to evaluate by trading off exploration and exploitation.\n",
    "\n",
    "Bayesian optimization utilizes the following **process**:\n",
    "\n",
    "1. Start with a handful of initial samples of different hyperparmeters and calculate the performance (e.g., the \"function\" that you want to optimize) for each combination. (**Note**: you can think of this as a small `RandomizedSearch`.)\n",
    "2. Fit a model mapping these initial hyperparemeter values to performance. (**Specifically**, the library we use utlizes **Tree-structured Parzen Estimators**.)\n",
    "3. We then use this model -- contructing what's called an **acuisition function** -- to change our parameters in a way that provides a better overall fit. Note that this is where things start to get really confusing!\n",
    "4. Repeat until you are satisfied.\n",
    "\n",
    "The **advantages** of Bayesian optimization over the hyperparameter optimization approaches taht we've discussed so far include:\n",
    "\n",
    "- Efficient for problems where function evaluations are expensive.\n",
    "- Reduces the number of evaluations needed compared to grid or random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian optimization in Python\n",
    "\n",
    "The details of Bayesian optimization are less important (at least for us) than the implementation, and the implementation in Python is pretty easy. There are a number of different libraries for Bayesian optimization in Python, but we are going to focus on the `hyperopt` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the necessary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll.base import scope # for controlling data types\n",
    "from sklearn.metrics import f1_score # for evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the `sklearn` functions, we start by setting up a dictionary with our \"hyperparameter space\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 1, 15, 1)),  # Integer values\n",
    "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 1, 15, 1)),  # Integer values\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, -2),  # Float values\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),  # Float values\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1),  # Float values\n",
    "    'n_estimators': scope.int(hp.quniform('n_estimators', 100, 1000, 1))  # Integer values\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference, however, is that we use various [probability distributions](http://hyperopt.github.io/hyperopt/getting-started/search_spaces/) to define the space (instead of hard-coding specific values). We can visualize what each of these distributions looks like using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "# Function to generate samples from a given hyperparameter distribution\n",
    "def sample_hyperparameter(hyperparameter, n_samples):\n",
    "    \"\"\"\n",
    "    Generate samples from a given hyperparameter distribution.\n",
    "\n",
    "    Args:\n",
    "        hyperparameter: A Hyperopt distribution (e.g., hp.uniform, hp.loguniform).\n",
    "        n_samples: Number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        A list of samples from the distribution.\n",
    "    \"\"\"\n",
    "    return [sample(hyperparameter) for _ in range(n_samples)]\n",
    "\n",
    "# Define the hyperparameter space\n",
    "space = {\n",
    "    'max_depth': hp.quniform('max_depth', 1, 15, 1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 15, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, -2),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1),\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 1000, 1)\n",
    "}\n",
    "\n",
    "# Number of samples to generate\n",
    "n_samples = 10000\n",
    "\n",
    "# Sample each hyperparameter using the function\n",
    "samples = {\n",
    "    'max_depth': [int(x) for x in sample_hyperparameter(space['max_depth'], n_samples)],\n",
    "    'min_child_weight': [int(x) for x in sample_hyperparameter(space['min_child_weight'], n_samples)],\n",
    "    'learning_rate': sample_hyperparameter(space['learning_rate'], n_samples),\n",
    "    'subsample': sample_hyperparameter(space['subsample'], n_samples),\n",
    "    'colsample_bytree': sample_hyperparameter(space['colsample_bytree'], n_samples),\n",
    "    'n_estimators': [int(x) for x in sample_hyperparameter(space['n_estimators'], n_samples)],\n",
    "}\n",
    "\n",
    "# Plot the distributions\n",
    "for param, values in samples.items():\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(values, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    plt.title(f\"Distribution of {param}\")\n",
    "    plt.xlabel(param)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define the objective function that we want to optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    # Cast parameters to proper types if necessary\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    \n",
    "    # Define the XGBoost model with the parameters\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    xgb_model.fit(df_train[features], df_train[y])\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = xgb_model.predict(df_test[features])\n",
    "    \n",
    "    # Calculate the F1 score (negative because we minimize in Hyperopt)\n",
    "    score = f1_score(df_test[y], y_pred)\n",
    "    \n",
    "    return {'loss': -score, 'status': STATUS_OK}  # Hyperopt minimizes loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we use the `fmin()` function to minimize this objective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Trials object to store intermediate results\n",
    "trials = Trials()\n",
    "\n",
    "# Run the Hyperopt optimization\n",
    "best_params = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Best set of hyperparameters: \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annoyingly, `fmin()` returns floating point numbers for parameters that we need to cast as integers. So we need to quickly turn these floats back into `int`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in best_params:\n",
    "    if key in ['max_depth', 'min_child_weight', 'n_estimators']:\n",
    "        best_params[key] = int(best_params[key])\n",
    "\n",
    "print(\"Best set of hyperparameters: \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this improve our performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's lower the learning rate\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', **best_params)\n",
    "\n",
    "# Fit the model\n",
    "xgb_model.fit(df_train[features], df_train[y])\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(df_test[features])\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(df_test[y], y_pred)\n",
    "print(f'Precision is {precision}')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(df_test[y], y_pred)\n",
    "print(f'Recall is {recall}')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(df_test[y], y_pred)\n",
    "print(f'F1 score is {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation with `xgboost`\n",
    "\n",
    "While the `xgboost` library has a built-in method (i.e., `xgboost.cv()`) for cross-validation, the list of metrics that are available to monitor performance is quite limited and I prefer the flexibility of using `sklearn` to perform cross-validation \"manually\". Let's see how to do this. First, we need to import the `KFold` function from `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier\n",
    "clf = xgb.XGBClassifier(objective='binary:logistic', **best_params)\n",
    "\n",
    "# Get the k folds\n",
    "kf = KFold(n_splits=10, shuffle = True, random_state=50)\n",
    "\n",
    "# Loop over folds and calculate performance measure\n",
    "results = []\n",
    "for k, (train_idx, test_idx) in enumerate(kf.split(df[features])):\n",
    "    # Fit model\n",
    "    cfit = clf.fit(df[features].iloc[train_idx], df[y].iloc[train_idx])\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = cfit.predict(df[features].iloc[test_idx])\n",
    "    \n",
    "    # Write results\n",
    "    result = {'fold': k,\n",
    "              'precision': precision_score(df[y].iloc[test_idx], y_pred),\n",
    "              'recall': recall_score(df[y].iloc[test_idx], y_pred),\n",
    "              'f1': f1_score(df[y].iloc[test_idx], y_pred)}\n",
    "    # If we want to monitor progress\n",
    "    print(result)\n",
    "              \n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average precision\n",
    "np.mean([x['precision'] for x in results])\n",
    "print(f'Average precision is {np.mean([x[\"precision\"] for x in results])}')\n",
    "\n",
    "# Average recall\n",
    "np.mean([x['recall'] for x in results])\n",
    "print(f'Average recall is {np.mean([x[\"recall\"] for x in results])}')\n",
    "\n",
    "# Average F1\n",
    "np.mean([x['f1'] for x in results])\n",
    "print(f'Average F1 is {np.mean([x[\"f1\"] for x in results])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like for our decision tree and random forest models, we can plot the most important features for our ``XGBoost`` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "xgb.plot_importance(xgb_model, importance_type='weight')  # 'weight' is the default\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our final model in \"production\"\n",
    "\n",
    "So we've have a model that we think is pretty good -- now what? We can now use our model to predict new data by, for instance, creating a \"would you have survived the Titanic app\". The steps for resusing our \"final\" model include:\n",
    "\n",
    "1. Fit the final model using **all** the data.\n",
    "2. `pickle` the model for later use\n",
    "\n",
    "And then when you want to predict a new observation, you:\n",
    "\n",
    "3. Read in the data and format it **exactly** how the training data was formatted. For us, that means reading in our data and storing it as a `pandas` dataframe with the following variables: ['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'female'].\n",
    "\n",
    "First, we fit and save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier\n",
    "clf = xgb.XGBClassifier(objective='binary:logistic', **best_params)\n",
    "\n",
    "# Fit on all data\n",
    "cfit = clf.fit(df[features], df[y])\n",
    "\n",
    "# Save the model\n",
    "import pickle\n",
    "pickle.dump(cfit, open('xgb_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then load the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('xgb_model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new example observation as a dictionary with the variable names as keys\n",
    "new_obs = {'PassengerId': 1,\n",
    "            'Survived': 0,\n",
    "            'Pclass': 3,\n",
    "            'Name': 'Braverman, Suella',\n",
    "            'Sex': 'Female', \n",
    "            'Age': 43.0,\n",
    "            'SibSp': 0,\n",
    "            'Parch': 0,\n",
    "            'Ticket': 'A/5 21171',\n",
    "            'Fare': 7.25,\n",
    "            'Cabin': '',\n",
    "            'Embarked': 'S',\n",
    "            'female': 1}\n",
    "\n",
    "# Convert to a dataframe\n",
    "df_new_obs = pd.DataFrame([new_obs])\n",
    "\n",
    "# Make a prediction\n",
    "prob = loaded_model.predict_proba(df_new_obs[features])\n",
    "print(f'Probability of survival is {prob[0][1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression using `xgboost`\n",
    "\n",
    "Solving regression problems with `xgboost` follow the same basic syntax. Let's start by importing our housing price data that we used to demonstrate regression analysis in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/housing.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And split the data into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, test_size=.3, random_state=42)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab some features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 'SalePrice'\n",
    "features = ['LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'GrLivArea', 'FullBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'GarageArea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup an `XGBREgressor()` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(objective='reg:squarederror')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and get out of sample predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Fit the model\n",
    "model.fit(data_train[features], data_train[y])\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(data_test[features])\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(data_test[y], y_pred))\n",
    "print(f'RMSE is {rmse}')\n",
    "\n",
    "# Calculate R^2\n",
    "r2 = r2_score(data_test[y], y_pred)\n",
    "print(f'R^2 is {r2}')\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(data_test[y], y_pred)\n",
    "print(f'MAE is {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features are the most important in our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "xgb.plot_importance(model, importance_type='weight')  # 'weight' is the default\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You try\n",
    "\n",
    "Can you use Bayesian optimization to find the best hyperparameters for our real estate regression model? Complete the following steps:\n",
    "\n",
    "1. Determine how your \"features\" (or variables) are measured. What type of probability distribution can you use to represent each variable?\n",
    "2. Set up your \"sample\" space using `hp`.\n",
    "3. Define your objective function. What is a good measure of performance for regression problems?\n",
    "4. Use `fmin()` to find the best parameters and print them.\n",
    "5. Fit the model using the best parameters. Does the out-of-sample performance increase?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
